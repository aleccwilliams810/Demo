{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0af694-d937-43a4-b347-869467b84654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a39918-9d09-4f26-9edb-fc3f479988fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataScraper:\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "        self.player_urls = [f'https://www.pro-football-reference.com/years/{year}/fantasy.htm' for year in years]\n",
    "        self.team_url = 'https://www.pro-football-reference.com/years/{}/'\n",
    "\n",
    "    def scrape_data(self, urls):\n",
    "        data_frames = []\n",
    "        for url in urls:\n",
    "            year = url.split('/')[-2]  # Extract the year from the URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            tables = soup.find_all('table')\n",
    "\n",
    "            df_list = [pd.read_html(str(tables[i]))[0] for i in range(min(len(tables), 2))]\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['Year'] = float(year)\n",
    "                data_frames.append(df)\n",
    "\n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    def scrape_player_data(self):\n",
    "        return self.scrape_data(self.player_urls)\n",
    "\n",
    "    def scrape_team_data(self):\n",
    "        team_urls = [self.team_url.format(year) for year in self.years]\n",
    "        team_data = self.scrape_data(team_urls)\n",
    "        return team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551911f-a955-48d5-abb8-bbd607af9ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    dictionary = {'New York Giants': 'NYG',\n",
    " 'Las Vegas Raiders': 'LVR',\n",
    " 'Los Angeles Chargers': 'LAC',\n",
    " 'Denver Broncos': 'DEN',\n",
    " 'Green Bay Packers': 'GNB',\n",
    " 'Jacksonville Jaguars': 'JAX',\n",
    " 'Washington Redskins': 'WAS',\n",
    " 'Los Angeles Rams': 'LAR',\n",
    " 'Arizona Cardinals': 'ARI',\n",
    " 'Carolina Panthers': 'CAR',\n",
    " 'Baltimore Ravens': 'BAL',\n",
    " 'New York Jets': 'NYJ',\n",
    " 'Miami Dolphins': 'MIA',\n",
    " 'Minnesota Vikings': 'MIN',\n",
    " 'Oakland Raiders': 'OAK',\n",
    " 'Chicago Bears': 'CHI',\n",
    " 'New England Patriots': 'NWE',\n",
    " 'Tennessee Titans': 'TEN',\n",
    " 'New Orleans Saints': 'NOR',\n",
    " 'Cleveland Browns': 'CLE',\n",
    " 'Tampa Bay Buccaneers': 'TAM',\n",
    " 'Buffalo Bills': 'BUF',\n",
    " 'Cincinnati Bengals': 'CIN',\n",
    " 'Houston Texans': 'HOU',\n",
    " 'San Francisco 49ers': 'SFO',\n",
    " 'Atlanta Falcons': 'ATL',\n",
    " 'Washington Football Team': 'WAS',\n",
    " 'Indianapolis Colts': 'IND',\n",
    " 'Seattle Seahawks': 'SEA',\n",
    " 'Pittsburgh Steelers': 'PIT',\n",
    " 'Dallas Cowboys': 'DAL',\n",
    " 'Detroit Lions': 'DET',\n",
    " 'Philadelphia Eagles': 'PHI',\n",
    " 'Kansas City Chiefs': 'KAN'}\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def try_convert_to_float(self, x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            if x == '' or pd.isna(x):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def convert_columns_to_float(self):\n",
    "        # Convert all columns to float if possible\n",
    "        self.data = self.data.applymap(self.try_convert_to_float)\n",
    "        self.data = self.data.astype(float, errors='ignore')\n",
    "\n",
    "    def flatten_multiindex_header(self):\n",
    "        # Flatten multi-level column index to single level column index if it exists\n",
    "        if isinstance(self.data.columns, pd.MultiIndex):\n",
    "            level1 = self.data.columns.get_level_values(0)\n",
    "            level2 = self.data.columns.get_level_values(1)\n",
    "\n",
    "            # Count duplicates in level2\n",
    "            duplicates = level2.value_counts() > 1\n",
    "\n",
    "            # Create new column names\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if duplicates[col_level2]:\n",
    "                    new_columns.append(f'{col_level1}_{col_level2}')\n",
    "                elif col_level2 == '':\n",
    "                    new_columns.append(col_level1)\n",
    "                else:\n",
    "                    new_columns.append(col_level2)\n",
    "\n",
    "            # Replace the MultiIndex header with the new flattened header\n",
    "            self.data.columns = new_columns\n",
    "        return self.data\n",
    " \n",
    "    def calculate_yards_per_attempt(self):\n",
    "    # Calculate yards per rushing attempt\n",
    "        if 'Rushing_Att' in self.data.columns and 'Rushing_Yds' in self.data.columns:\n",
    "            self.data['Y/A'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rushing_Att']) or x['Rushing_Att'] == 0) else x['Rushing_Yds'] / x['Rushing_Att'], axis=1)\n",
    "\n",
    "    def calculate_yards_per_reception(self):\n",
    "    # Calculate yards per reception\n",
    "        if 'Rec' in self.data.columns and 'Receiving_Yds' in self.data.columns:\n",
    "            self.data['Y/R'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rec']) or x['Rec'] == 0) else x['Receiving_Yds'] / x['Rec'], axis=1)\n",
    "            \n",
    "    def handle_missing_values(self, thresh=0.5):\n",
    "    \n",
    "        # Drop rows where most of the columns have string data\n",
    "        self.data = self.data[~(np.sum(np.vectorize(isinstance)(self.data.values, str), axis=1) > thresh * len(self.data.columns))]\n",
    "\n",
    "        def should_fill(col):\n",
    "            na_and_zero_count = (col.apply(lambda x: pd.isna(x) or x == 0)).sum()\n",
    "            return na_and_zero_count / len(col) >= 0.7\n",
    "\n",
    "        should_fill_mask = self.data.apply(should_fill)\n",
    "        fill_mask = self.data.apply(lambda col: col.isna() & should_fill_mask[col.name])\n",
    "        self.data = self.data.mask(fill_mask, 0.0)\n",
    "\n",
    "        # Drop columns where most of the rows are null\n",
    "        self.data = self.data.drop(columns=self.data.columns[self.data.isnull().mean() > thresh], errors='ignore')\n",
    "\n",
    "        # Drop rows where most of the columns are null\n",
    "        self.data = self.data.dropna(thresh=thresh * len(self.data.columns))\n",
    "\n",
    "        # Drop rows where PPR data is null (if present in dataframe)\n",
    "        if 'PPR' in self.data.columns:\n",
    "            self.data = self.data.dropna(subset=['PPR'])\n",
    "\n",
    "         # Drop rows where Rk > 350 if Rk is a column\n",
    "        if 'Rk' in self.data.columns:\n",
    "            self.data = self.data[self.data['Rk'] <= 400]\n",
    "\n",
    "    def replace_team_names(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            mask = self.data['Tm'].str.startswith(key)\n",
    "            if mask.any():\n",
    "                self.data.loc[mask, 'Tm'] = value\n",
    "        return self.data\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        # Create new features or modify existing ones based on domain knowledge\n",
    "        pass\n",
    "\n",
    "    def feature_scaling(self):\n",
    "        # Scale numerical features to a standard range, e.g., using Min-Max scaling or StandardScaler from sklearn\n",
    "        pass\n",
    "\n",
    "    def normalize_data(self):\n",
    "        # Normalize data to reduce the impact of outliers or skewed distributions, e.g., using log transformation or Box-Cox transformation\n",
    "        pass\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.convert_columns_to_float()\n",
    "        self.flatten_multiindex_header()\n",
    "        self.handle_missing_values()\n",
    "        self.replace_team_names(dictionary=self.dictionary)  # pass dictionary argument\n",
    "        self.calculate_yards_per_attempt()\n",
    "        self.calculate_yards_per_reception()\n",
    "        # Call other preprocessing methods in the correct order\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80ff8c-54cd-40e7-a6c1-4da66f8a26d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MergeAndProcess:\n",
    "    def __init__(self, player_data, team_data):\n",
    "        self.player_data = player_data\n",
    "        self.team_data = team_data\n",
    "\n",
    "    def merge(self):\n",
    "        # Create a dictionary to map team data to new columns in player data\n",
    "        team_data_map = {}\n",
    "        for col in self.team_data.columns:\n",
    "            if col not in ['Year', 'Tm']:\n",
    "                if col in self.player_data.columns:\n",
    "                    team_data_map[col] = col\n",
    "                else:\n",
    "                    team_data_map[col] = f'Team_{col}'\n",
    "\n",
    "        # Create new columns in player data for team data\n",
    "        for col in team_data_map.values():\n",
    "            self.player_data[col] = np.nan\n",
    "\n",
    "        # Map team data to new columns in player data based on team and year\n",
    "        self.player_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.team_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.player_data.update(self.team_data.rename(columns=team_data_map))\n",
    "\n",
    "        # Flatten the column index\n",
    "        if isinstance(self.player_data.columns, pd.MultiIndex):\n",
    "            level1 = self.player_data.columns.get_level_values(0)\n",
    "            level2 = self.player_data.columns.get_level_values(1)\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if col_level1.startswith('Team_'):\n",
    "                    new_columns.append(col_level2)\n",
    "                else:\n",
    "                    new_columns.append(col_level1)\n",
    "            self.player_data.columns = new_columns\n",
    "\n",
    "        return self.player_data.reset_index()\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        # Merge player and team data\n",
    "        merged_data = self.merge()\n",
    "        \n",
    "        # Add next year PPR\n",
    "        merged_data['next_year_PPR'] = merged_data.groupby('Player')['PPR'].shift(-1)\n",
    "\n",
    "        # Add PPR per game\n",
    "        merged_data['PPR_per_game'] = np.nan\n",
    "        try:\n",
    "            merged_data['PPR_per_game'] = merged_data['PPR'] / merged_data['G']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Convert column types\n",
    "        for col in merged_data.columns:\n",
    "            if merged_data[col].dtype == 'object':\n",
    "                try:\n",
    "                    merged_data[col] = pd.to_numeric(merged_data[col], errors='raise').astype('float')\n",
    "                except ValueError:\n",
    "                    merged_data[col] = merged_data[col].astype('string')\n",
    "            else:\n",
    "                dtype = merged_data[col].dtype\n",
    "                merged_data[col] = merged_data[col].astype(dtype)\n",
    "\n",
    "        # Drop missing values\n",
    "        merged_data = merged_data.dropna()\n",
    "        \n",
    "        # Reset index\n",
    "        merged_data = merged_data.reset_index(drop=True)\n",
    "        \n",
    "        # Replace non-alphanumeric characters in player names\n",
    "        merged_data['Player'] = merged_data['Player'].str.replace(r'[^\\w\\s]+', '')\n",
    "        \n",
    "        return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77815273-1c01-40d0-804a-6515cf0825fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = list(range(2013, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6f45c-4d9c-4d37-a74c-cad5733bc734",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper = DataScraper(years)\n",
    "player_data = scraper.scrape_player_data()\n",
    "team_data = scraper.scrape_team_data()\n",
    "\n",
    "player_preprocessor = DataPreprocessor(player_data)\n",
    "player_data = player_preprocessor.preprocess_data()\n",
    "\n",
    "team_preprocessor = DataPreprocessor(team_data)\n",
    "team_data = team_preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2f767-1094-477c-9268-662d8f97d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create instance of MergeAndProcess class\n",
    "merger = MergeAndProcess(player_data, team_data)\n",
    "\n",
    "# call process_data method to merge and process data\n",
    "merged_data = merger.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4469baf-6612-499d-bea5-49d68381397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the merged_data DataFrame into training/validation (-2020) and testing (2021)\n",
    "train_val_data = merged_data[merged_data['Year'] < merged_data['Year'].max()]\n",
    "test_data = merged_data[merged_data['Year'] == merged_data['Year'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46050817-a346-4183-bc79-97c4358207fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
