{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1170ea43-6d36-4d6e-82c6-d7b94e9cc445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PFR_Scraper_fantasy_teams import DataScraper \n",
    "from PFR_Processor_fantasy_teams import DataPreprocessor, wrapper\n",
    "from PFR_Merger_fantasy_teams import MergeAndProcess\n",
    "from PFR_Model_Builder_fantasy_teams import create_and_evaluate_model, create_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfad3f74-ee43-4472-8489-5d53e381d911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = list(range(2010, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5788b32-9290-4750-a8f8-f9d585cda848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scraper \u001b[38;5;241m=\u001b[39m DataScraper(years)\n\u001b[0;32m----> 2\u001b[0m player_data \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_player_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m team_data \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mscrape_team_data()\n",
      "File \u001b[0;32m~/Coding/GitHub/Practice/PFR_Scraper_fantasy_teams.py:34\u001b[0m, in \u001b[0;36mDataScraper.scrape_player_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     33\u001b[0m     player_data_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplayer_urls))\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mplayer_data_frames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "scraper = DataScraper(years)\n",
    "player_data = scraper.scrape_player_data()\n",
    "team_data = scraper.scrape_team_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37d779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player_preprocessor = DataPreprocessor(player_data)\n",
    "player_data = player_preprocessor.preprocess_data()\n",
    "team_preprocessor = DataPreprocessor(team_data)\n",
    "team_data = team_preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940895fe-aaba-4080-b6fd-cf8393079d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create instance of MergeAndProcess class\n",
    "merger = MergeAndProcess(player_data, team_data)\n",
    "\n",
    "# call process to merge and process data\n",
    "merged_data = merger.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65d4e9-668a-4bc2-9bba-ea8a876efb10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "num_data = merged_data.select_dtypes(include=[np.number])\n",
    "scaled = scaler.fit_transform(num_data)\n",
    "scaled_df = pd.DataFrame(scaled, columns=num_data.columns)\n",
    "\n",
    "# Copy original to keep str data\n",
    "merged_data_scaled = merged_data.copy()\n",
    "\n",
    "# Replace num columns with standardized data, except for 'Year'\n",
    "for column in scaled_df.columns:\n",
    "    if column != 'Year':\n",
    "        merged_data_scaled[column] = scaled_df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac7ebb-8fef-45a3-9bf2-bd0465e60f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get pos from the 'FantPos' column\n",
    "positions = merged_data_scaled['FantPos'].unique()\n",
    "\n",
    "# Create empty dict to store models for each pos\n",
    "models = {}\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [100, 200, 300],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'dropout_rate': [0.1, 0.4, 0.6]\n",
    "}\n",
    "\n",
    "# Define num features to select w RFE\n",
    "n_features = 15\n",
    "\n",
    "# Define num folds for cv\n",
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf47e65-7600-4755-9d12-ea842fcf8384",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the arguments for the pool.map function\n",
    "args_list = [(pos, merged_data_scaled, param_grid, n_folds, n_features, {'dropout_rate': 0.1}) for pos in positions]\n",
    "\n",
    "# Create a pool and run the function in parallel\n",
    "n_jobs = 2  # Number of parallel processes\n",
    "with Pool(processes=n_jobs) as pool:\n",
    "    results = pool.map(create_and_evaluate_model, args_list)\n",
    "\n",
    "# Create a dictionary of models\n",
    "models = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1544cbf-7e8f-4ce5-8a66-64a8bc2eac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be3838-8224-4bea-9d5b-6708449af1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4d0a4-0958-44cb-a2bc-a078f919fb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269618b2-9681-471f-9ed8-0b6e0858e03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3642e9-36fc-40ec-9d28-474642850fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a0f31-b89b-4eef-bff0-6594dcfaf2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96f11c-5e3e-4f72-af10-f396b561cc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through positions and create model for each\n",
    "for pos in positions:\n",
    "    print('Creating model for position:', pos)\n",
    "    \n",
    "    # Filter for training (< 2020) and for current pos\n",
    "    train_data = merged_data_scaled[(merged_data_scaled['Year'] < 2020) & (merged_data_scaled['FantPos'] == pos)]\n",
    "\n",
    "    # Filter for testing (< 2022 and not in training) and for current pos\n",
    "    test_data = merged_data_scaled[(merged_data_scaled['Year'] < 2022) & (~merged_data_scaled.index.isin(train_data.index)) & (merged_data_scaled['FantPos'] == pos)]\n",
    "\n",
    "    # Select input and outputs\n",
    "    X_train = train_data.drop(['next_year_PPR', 'Year'], axis=1)\n",
    "    X_train = X_train.select_dtypes(include=[np.number])  \n",
    "    y_train = train_data['next_year_PPR']\n",
    "\n",
    "    X_test = test_data.drop(['next_year_PPR', 'Year'], axis=1)\n",
    "    X_test = X_test.select_dtypes(include=[np.number]) \n",
    "    y_test = test_data['next_year_PPR']\n",
    "\n",
    "    # linear regression object\n",
    "    lin_reg = LinearRegression()\n",
    "\n",
    "    # Create RFE and specify the num of features to select\n",
    "    rfe = RFE(lin_reg, n_features_to_select=n_features)\n",
    "\n",
    "    # Fit RFE to training data\n",
    "    rfe.fit(X_train, y_train)\n",
    "\n",
    "    # Get selected features\n",
    "    feature_index = rfe.get_support(indices=True)\n",
    "    print(f'Selected features for {pos}: {feature_index}')\n",
    "\n",
    "    # Filter training and testing data\n",
    "    X_train = X_train.iloc[:, feature_index]\n",
    "    X_test = X_test.iloc[:, feature_index]\n",
    "\n",
    "    # Convert w np.array\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Create a KerasRegressor with the current model architecture\n",
    "    model = KerasRegressor(model=create_model, dropout_rate=0.1, verbose=0)\n",
    "\n",
    "    # Create a GridSearchCV object with the hyperparameters to tune and the cross-validation folds\n",
    "    grid = GridSearchCV(estimator=model, param_grid={\n",
    "        'input_shape': [X_train.shape[1:]],\n",
    "        'neurons': [32, 64, 128],\n",
    "        'dropout_rate': [0.1, 0.2],\n",
    "        'optimizer': ['adam', 'rmsprop']\n",
    "    }, cv=n_folds, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Fit the GridSearchCV object to the training data\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding mean squared error score\n",
    "    best_params = grid_result.best_params_\n",
    "    best_score = -grid_result.best_score_\n",
    "\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best MSE:\", best_score)\n",
    "\n",
    "    # Create the final model with the best hyperparameters and fit it to the training data\n",
    "    final_model = create_model(dropout_rate=best_params['dropout_rate'], optimizer=best_params['optimizer'])\n",
    "    final_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n",
    "\n",
    "    # Evaluate the final model on the test data\n",
    "    mse, mae = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Mean Squared Error for test data:\", mse)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "    # Add the final model to the dictionary of models\n",
    "    models[pos] = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f49f5-9d08-4c4f-ac90-23d4b900ef24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate over the models dictionary\n",
    "for model_name, model in models.items():\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculate R-squared\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print the r2 score for each model\n",
    "    print(f\"R-squared for {model_name}: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e304914-70d3-4a19-8944-699d40ad4ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the figure and axes\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Loop through each position and make predictions on the test data\n",
    "for i, (position, model) in enumerate(models.items()):\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Plot the predicted values against the actual values\n",
    "    axs[i].scatter(y_test, y_pred)\n",
    "    axs[i].set_xlabel(\"Actual\")\n",
    "    axs[i].set_ylabel(\"Predicted\")\n",
    "    axs[i].set_title(\"Predicted vs Actual for \" + position)\n",
    "\n",
    "# Adjust the spacing and layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de61db7-e548-4eed-900f-60c18a6713af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7924b-346d-4653-bd84-7ac4da10091f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
