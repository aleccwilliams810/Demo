{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3274708e-6a74-43e8-831f-228ab2b7d2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a39918-9d09-4f26-9edb-fc3f479988fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataScraper:\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "        self.player_urls = [f'https://www.pro-football-reference.com/years/{year}/fantasy.htm' for year in years]\n",
    "        self.team_url = 'https://www.pro-football-reference.com/years/{}/'\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def scrape_data(self, url):\n",
    "        response = self.session.get(url)\n",
    "        response_text = response.text\n",
    "\n",
    "        parse_only = SoupStrainer('table')\n",
    "        soup = BeautifulSoup(response_text, 'lxml', parse_only=parse_only)\n",
    "        tables = soup.find_all('table')\n",
    "\n",
    "        df_list = [pd.read_html(str(tables[i]))[0] for i in range(min(len(tables), 2))]\n",
    "        if df_list:\n",
    "            year = url.split('/')[-2]  # Extract the year from the URL\n",
    "            df = pd.concat(df_list)\n",
    "            df['Year'] = str(year)\n",
    "            return df\n",
    "\n",
    "    def scrape_player_data(self):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            player_data_frames = list(executor.map(self.scrape_data, self.player_urls))\n",
    "        return pd.concat([df for df in player_data_frames if df is not None], ignore_index=True)\n",
    "\n",
    "    def scrape_team_data(self):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            team_urls = [self.team_url.format(year) for year in self.years]\n",
    "            team_data_frames = list(executor.map(self.scrape_data, team_urls))\n",
    "        return pd.concat([df for df in team_data_frames if df is not None], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0551911f-a955-48d5-abb8-bbd607af9ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    dictionary = {'New York Giants': 'NYG',\n",
    " 'Las Vegas Raiders': 'LVR',\n",
    " 'Los Angeles Chargers': 'LAC',\n",
    " 'Denver Broncos': 'DEN',\n",
    " 'Green Bay Packers': 'GNB',\n",
    " 'Jacksonville Jaguars': 'JAX',\n",
    " 'Washington Redskins': 'WAS',\n",
    " 'Los Angeles Rams': 'LAR',\n",
    " 'Arizona Cardinals': 'ARI',\n",
    " 'Carolina Panthers': 'CAR',\n",
    " 'Baltimore Ravens': 'BAL',\n",
    " 'New York Jets': 'NYJ',\n",
    " 'Miami Dolphins': 'MIA',\n",
    " 'Minnesota Vikings': 'MIN',\n",
    " 'Oakland Raiders': 'OAK',\n",
    " 'Chicago Bears': 'CHI',\n",
    " 'New England Patriots': 'NWE',\n",
    " 'Tennessee Titans': 'TEN',\n",
    " 'New Orleans Saints': 'NOR',\n",
    " 'Cleveland Browns': 'CLE',\n",
    " 'Tampa Bay Buccaneers': 'TAM',\n",
    " 'Buffalo Bills': 'BUF',\n",
    " 'Cincinnati Bengals': 'CIN',\n",
    " 'Houston Texans': 'HOU',\n",
    " 'San Francisco 49ers': 'SFO',\n",
    " 'Atlanta Falcons': 'ATL',\n",
    " 'Washington Football Team': 'WAS',\n",
    " 'Indianapolis Colts': 'IND',\n",
    " 'Seattle Seahawks': 'SEA',\n",
    " 'Pittsburgh Steelers': 'PIT',\n",
    " 'Dallas Cowboys': 'DAL',\n",
    " 'Detroit Lions': 'DET',\n",
    " 'Philadelphia Eagles': 'PHI',\n",
    " 'Kansas City Chiefs': 'KAN'}\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def try_convert_to_float(self, x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            if x == '' or pd.isna(x):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def convert_columns_to_float(self):\n",
    "        # Convert all columns to float if possible\n",
    "        self.data = self.data.applymap(self.try_convert_to_float)\n",
    "        self.data = self.data.astype(float, errors='ignore')\n",
    "\n",
    "    def flatten_multiindex_header(self):\n",
    "        # Flatten multi-level column index to single level column index if it exists\n",
    "        if isinstance(self.data.columns, pd.MultiIndex):\n",
    "            level1 = self.data.columns.get_level_values(0)\n",
    "            level2 = self.data.columns.get_level_values(1)\n",
    "\n",
    "            # Count duplicates in level2\n",
    "            duplicates = level2.value_counts() > 1\n",
    "\n",
    "            # Create new column names\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if duplicates[col_level2]:\n",
    "                    new_columns.append(f'{col_level1}_{col_level2}')\n",
    "                elif col_level2 == '':\n",
    "                    new_columns.append(col_level1)\n",
    "                else:\n",
    "                    new_columns.append(col_level2)\n",
    "\n",
    "            # Replace the MultiIndex header with the new flattened header\n",
    "            self.data.columns = new_columns\n",
    "        return self.data\n",
    " \n",
    "    def calculate_yards_per_attempt(self):\n",
    "    # Calculate yards per rushing attempt\n",
    "        if 'Rushing_Att' in self.data.columns and 'Rushing_Yds' in self.data.columns:\n",
    "            self.data['Y/A'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rushing_Att']) or x['Rushing_Att'] == 0) else x['Rushing_Yds'] / x['Rushing_Att'], axis=1)\n",
    "\n",
    "    def calculate_yards_per_reception(self):\n",
    "    # Calculate yards per reception\n",
    "        if 'Rec' in self.data.columns and 'Receiving_Yds' in self.data.columns:\n",
    "            self.data['Y/R'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rec']) or x['Rec'] == 0) else x['Receiving_Yds'] / x['Rec'], axis=1)\n",
    "            \n",
    "    def handle_missing_values(self, thresh=0.5):\n",
    "    \n",
    "        # Drop rows where most of the columns have string data\n",
    "        self.data = self.data[~(np.sum(np.vectorize(isinstance)(self.data.values, str), axis=1) > thresh * len(self.data.columns))]\n",
    "\n",
    "        def should_fill(col):\n",
    "            na_and_zero_count = (col.apply(lambda x: pd.isna(x) or x == 0)).sum()\n",
    "            return na_and_zero_count / len(col) >= 0.7\n",
    "\n",
    "        should_fill_mask = self.data.apply(should_fill)\n",
    "        fill_mask = self.data.apply(lambda col: col.isna() & should_fill_mask[col.name])\n",
    "        self.data = self.data.mask(fill_mask, 0.0)\n",
    "\n",
    "        # Drop columns where most of the rows are null\n",
    "        self.data = self.data.drop(columns=self.data.columns[self.data.isnull().mean() > thresh], errors='ignore')\n",
    "\n",
    "        # Drop rows where most of the columns are null\n",
    "        self.data = self.data.dropna(thresh=thresh * len(self.data.columns))\n",
    "\n",
    "        # Drop rows where PPR data is null (if present in dataframe)\n",
    "        if 'PPR' in self.data.columns:\n",
    "            self.data = self.data.dropna(subset=['PPR'])\n",
    "\n",
    "         # Drop rows where Rk > 350 if Rk is a column\n",
    "        if 'Rk' in self.data.columns:\n",
    "            self.data = self.data[self.data['Rk'] <= 400]\n",
    "\n",
    "    def replace_team_names(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            mask = self.data['Tm'].str.startswith(key)\n",
    "            if mask.any():\n",
    "                self.data.loc[mask, 'Tm'] = value\n",
    "        return self.data\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        self.convert_columns_to_float()\n",
    "        self.flatten_multiindex_header()\n",
    "        self.handle_missing_values()\n",
    "        self.replace_team_names(dictionary=self.dictionary)  # pass dictionary argument\n",
    "        self.calculate_yards_per_attempt()\n",
    "        self.calculate_yards_per_reception()\n",
    "        # Call other preprocessing methods in the correct order\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d80ff8c-54cd-40e7-a6c1-4da66f8a26d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MergeAndProcess:\n",
    "    def __init__(self, player_data, team_data):\n",
    "        self.player_data = player_data\n",
    "        self.team_data = team_data\n",
    "\n",
    "    def merge(self):\n",
    "        # Create a dictionary to map team data to new columns in player data\n",
    "        team_data_map = {}\n",
    "        for col in self.team_data.columns:\n",
    "            if col not in ['Year', 'Tm']:\n",
    "                if col in self.player_data.columns:\n",
    "                    team_data_map[col] = col\n",
    "                else:\n",
    "                    team_data_map[col] = f'Team_{col}'\n",
    "\n",
    "        # Create new columns in player data for team data\n",
    "        for col in team_data_map.values():\n",
    "            self.player_data[col] = np.nan\n",
    "\n",
    "        # Map team data to new columns in player data based on team and year\n",
    "        self.player_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.team_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.player_data.update(self.team_data.rename(columns=team_data_map))\n",
    "\n",
    "        # Flatten the column index\n",
    "        if isinstance(self.player_data.columns, pd.MultiIndex):\n",
    "            level1 = self.player_data.columns.get_level_values(0)\n",
    "            level2 = self.player_data.columns.get_level_values(1)\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if col_level1.startswith('Team_'):\n",
    "                    new_columns.append(col_level2)\n",
    "                else:\n",
    "                    new_columns.append(col_level1)\n",
    "            self.player_data.columns = new_columns\n",
    "\n",
    "        return self.player_data.reset_index()\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        # Merge player and team data\n",
    "        merged_data = self.merge()\n",
    "        \n",
    "        # Add next year PPR\n",
    "        merged_data['next_year_PPR'] = merged_data.groupby('Player')['PPR'].shift(-1)\n",
    "\n",
    "        # Add PPR per game\n",
    "        merged_data['PPR_per_game'] = np.nan\n",
    "        try:\n",
    "            merged_data['PPR_per_game'] = merged_data['PPR'] / merged_data['G']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Convert column types\n",
    "        for col in merged_data.columns:\n",
    "            if merged_data[col].dtype == 'object':\n",
    "                try:\n",
    "                    merged_data[col] = pd.to_numeric(merged_data[col], errors='raise').astype('float')\n",
    "                except ValueError:\n",
    "                    merged_data[col] = merged_data[col].astype('string')\n",
    "            else:\n",
    "                dtype = merged_data[col].dtype\n",
    "                merged_data[col] = merged_data[col].astype(dtype)\n",
    "\n",
    "        # Drop missing values\n",
    "        merged_data = merged_data.dropna()\n",
    "        \n",
    "        # Reset index\n",
    "        merged_data = merged_data.reset_index(drop=True)\n",
    "        \n",
    "        # Replace non-alphanumeric characters in player names\n",
    "        merged_data['Player'] = merged_data['Player'].str.replace(r'[^\\w\\s]+', '')\n",
    "        \n",
    "        return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77815273-1c01-40d0-804a-6515cf0825fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = list(range(2013, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee09d111-24dc-4402-915b-ba92963d107a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper = DataScraper(years)\n",
    "player_data = scraper.scrape_player_data()\n",
    "team_data = scraper.scrape_team_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92392c3-56d2-4c09-b215-5a7544b1fbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "player_preprocessor = DataPreprocessor(player_data)\n",
    "player_data = player_preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d2e031-61c9-4bc8-8a24-70c54c27abe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "team_preprocessor = DataPreprocessor(team_data)\n",
    "team_data = team_preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc2f767-1094-477c-9268-662d8f97d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/kw6knpvn0cg26fb0nyg8brp40000gn/T/ipykernel_20757/2492068452.py:72: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  merged_data['Player'] = merged_data['Player'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "# create instance of MergeAndProcess class\n",
    "merger = MergeAndProcess(player_data, team_data)\n",
    "\n",
    "# call process_data method to merge and process data\n",
    "merged_data = merger.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4469baf-6612-499d-bea5-49d68381397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the merged_data DataFrame into training/validation (-2020) and testing (2021)\n",
    "train_val_data = merged_data[merged_data['Year'] < merged_data['Year'].max()]\n",
    "test_data = merged_data[merged_data['Year'] == merged_data['Year'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46050817-a346-4183-bc79-97c4358207fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data.to_csv('merged_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64ea11-c1d5-4d41-b74c-c7c3a9e92982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
