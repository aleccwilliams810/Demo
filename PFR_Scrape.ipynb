{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76531f79-3a38-4cc7-8b2b-78de46ac6461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a39918-9d09-4f26-9edb-fc3f479988fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataScraper:\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "        self.player_urls = [f'https://www.pro-football-reference.com/years/{year}/fantasy.htm' for year in years]\n",
    "        self.team_url = 'https://www.pro-football-reference.com/years/{}/'\n",
    "\n",
    "    def scrape_data(self, urls):\n",
    "        data_frames = []\n",
    "        for url in urls:\n",
    "            year = url.split('/')[-2]  # Extract the year from the URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            tables = soup.find_all('table')\n",
    "\n",
    "            df_list = [pd.read_html(str(tables[i]))[0] for i in range(min(len(tables), 2))]\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['Year'] = float(year)\n",
    "                data_frames.append(df)\n",
    "\n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    def scrape_player_data(self):\n",
    "        return self.scrape_data(self.player_urls)\n",
    "\n",
    "    def scrape_team_data(self):\n",
    "        team_urls = [self.team_url.format(year) for year in self.years]\n",
    "        team_data = self.scrape_data(team_urls)\n",
    "        return team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551911f-a955-48d5-abb8-bbd607af9ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    dictionary = {'New York Giants': 'NYG',\n",
    " 'Las Vegas Raiders': 'LVR',\n",
    " 'Los Angeles Chargers': 'LAC',\n",
    " 'Denver Broncos': 'DEN',\n",
    " 'Green Bay Packers': 'GNB',\n",
    " 'Jacksonville Jaguars': 'JAX',\n",
    " 'Washington Redskins': 'WAS',\n",
    " 'Los Angeles Rams': 'LAR',\n",
    " 'Arizona Cardinals': 'ARI',\n",
    " 'Carolina Panthers': 'CAR',\n",
    " 'Baltimore Ravens': 'BAL',\n",
    " 'New York Jets': 'NYJ',\n",
    " 'Miami Dolphins': 'MIA',\n",
    " 'Minnesota Vikings': 'MIN',\n",
    " 'Oakland Raiders': 'OAK',\n",
    " 'Chicago Bears': 'CHI',\n",
    " 'New England Patriots': 'NWE',\n",
    " 'Tennessee Titans': 'TEN',\n",
    " 'New Orleans Saints': 'NOR',\n",
    " 'Cleveland Browns': 'CLE',\n",
    " 'Tampa Bay Buccaneers': 'TAM',\n",
    " 'Buffalo Bills': 'BUF',\n",
    " 'Cincinnati Bengals': 'CIN',\n",
    " 'Houston Texans': 'HOU',\n",
    " 'San Francisco 49ers': 'SFO',\n",
    " 'Atlanta Falcons': 'ATL',\n",
    " 'Washington Football Team': 'WAS',\n",
    " 'Indianapolis Colts': 'IND',\n",
    " 'Seattle Seahawks': 'SEA',\n",
    " 'Pittsburgh Steelers': 'PIT',\n",
    " 'Dallas Cowboys': 'DAL',\n",
    " 'Detroit Lions': 'DET',\n",
    " 'Philadelphia Eagles': 'PHI',\n",
    " 'Kansas City Chiefs': 'KAN'}\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def try_convert_to_float(self, x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            if x == '' or pd.isna(x):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def convert_columns_to_float(self):\n",
    "        # Convert all columns to float if possible\n",
    "        self.data = self.data.applymap(self.try_convert_to_float)\n",
    "        self.data = self.data.astype(float, errors='ignore')\n",
    "\n",
    "    def flatten_multiindex_header(self):\n",
    "        # Flatten multi-level column index to single level column index if it exists\n",
    "        if isinstance(self.data.columns, pd.MultiIndex):\n",
    "            level1 = self.data.columns.get_level_values(0)\n",
    "            level2 = self.data.columns.get_level_values(1)\n",
    "\n",
    "            # Count duplicates in level2\n",
    "            duplicates = level2.value_counts() > 1\n",
    "\n",
    "            # Create new column names\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if duplicates[col_level2]:\n",
    "                    new_columns.append(f'{col_level1}_{col_level2}')\n",
    "                elif col_level2 == '':\n",
    "                    new_columns.append(col_level1)\n",
    "                else:\n",
    "                    new_columns.append(col_level2)\n",
    "\n",
    "            # Replace the MultiIndex header with the new flattened header\n",
    "            self.data.columns = new_columns\n",
    "        return self.data\n",
    " \n",
    "    def calculate_yards_per_attempt(self):\n",
    "    # Calculate yards per rushing attempt\n",
    "        if 'Rushing_Att' in self.data.columns and 'Rushing_Yds' in self.data.columns:\n",
    "            self.data['Y/A'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rushing_Att']) or x['Rushing_Att'] == 0) else x['Rushing_Yds'] / x['Rushing_Att'], axis=1)\n",
    "\n",
    "    def calculate_yards_per_reception(self):\n",
    "    # Calculate yards per reception\n",
    "        if 'Rec' in self.data.columns and 'Receiving_Yds' in self.data.columns:\n",
    "            self.data['Y/R'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rec']) or x['Rec'] == 0) else x['Receiving_Yds'] / x['Rec'], axis=1)\n",
    "            \n",
    "    def handle_missing_values(self, thresh=0.5):\n",
    "        \n",
    "         # Drop columns where most of the rows are null\n",
    "        self.data = self.data.drop(columns=self.data.columns[self.data.isnull().mean() > thresh], errors='ignore')\n",
    "\n",
    "        # Drop rows where most of the columns are null\n",
    "        self.data = self.data.dropna(thresh=thresh*len(self.data.columns))\n",
    "\n",
    "        # Drop rows where PPR data is null (if present in dataframe)\n",
    "        if 'PPR' in self.data.columns:\n",
    "            self.data = self.data.dropna(subset=['PPR'])\n",
    "\n",
    "        # Drop rows where most of the columns have string data\n",
    "        self.data = self.data[~(np.sum(np.vectorize(isinstance)(self.data.values, str), axis=1) > thresh*len(self.data.columns))]\n",
    "    \n",
    "    def replace_team_names(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            mask = self.data['Tm'].str.startswith(key)\n",
    "            if mask.any():\n",
    "                self.data.loc[mask, 'Tm'] = value\n",
    "        return self.data\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        # Create new features or modify existing ones based on domain knowledge\n",
    "        pass\n",
    "\n",
    "    def feature_scaling(self):\n",
    "        # Scale numerical features to a standard range, e.g., using Min-Max scaling or StandardScaler from sklearn\n",
    "        pass\n",
    "\n",
    "    def normalize_data(self):\n",
    "        # Normalize data to reduce the impact of outliers or skewed distributions, e.g., using log transformation or Box-Cox transformation\n",
    "        pass\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.convert_columns_to_float()\n",
    "        self.flatten_multiindex_header()\n",
    "        self.handle_missing_values()\n",
    "        self.replace_team_names(dictionary=self.dictionary)  # pass dictionary argument\n",
    "        self.calculate_yards_per_attempt()\n",
    "        self.calculate_yards_per_reception()\n",
    "        # Call other preprocessing methods in the correct order\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76980cc-0206-4df7-959e-0eea1026cd0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MergeData:\n",
    "    def __init__(self, player_data, team_data):\n",
    "        self.player_data = player_data\n",
    "        self.team_data = team_data\n",
    "\n",
    "    def add_team_data(self):\n",
    "        # Create a dictionary to map team data to new columns in player data\n",
    "        team_data_map = {}\n",
    "        for col in self.team_data.columns:\n",
    "            if col not in ['Year', 'Tm']:\n",
    "                if col in self.player_data.columns:\n",
    "                    team_data_map[col] = col\n",
    "                else:\n",
    "                    team_data_map[col] = f'Team_{col}'\n",
    "\n",
    "        # Create new columns in player data for team data\n",
    "        for col in team_data_map.values():\n",
    "            self.player_data[col] = np.nan\n",
    "\n",
    "        # Map team data to new columns in player data based on team and year\n",
    "        self.player_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.team_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.player_data.update(self.team_data.rename(columns=team_data_map))\n",
    "\n",
    "        # Flatten the column index\n",
    "        if isinstance(self.player_data.columns, pd.MultiIndex):\n",
    "            level1 = self.player_data.columns.get_level_values(0)\n",
    "            level2 = self.player_data.columns.get_level_values(1)\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if col_level1.startswith('Team_'):\n",
    "                    new_columns.append(col_level2)\n",
    "                else:\n",
    "                    new_columns.append(col_level1)\n",
    "            self.player_data.columns = new_columns\n",
    "\n",
    "        return self.player_data.reset_index()\n",
    "\n",
    "    def add_and_flatten(self):\n",
    "        self.add_team_data()\n",
    "        #self._flatten_multiindex_header()\n",
    "        \n",
    "        return self.player_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77815273-1c01-40d0-804a-6515cf0825fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = list(range(2017, 2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa75c8-ae56-4cc2-9606-732fbfc61e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper = DataScraper(years)\n",
    "player_data = scraper.scrape_player_data()\n",
    "team_data = scraper.scrape_team_data()\n",
    "\n",
    "player_preprocessor = DataPreprocessor(player_data)\n",
    "player_data = player_preprocessor.preprocess_data()\n",
    "\n",
    "team_preprocessor = DataPreprocessor(team_data)\n",
    "team_data = team_preprocessor.preprocess_data()\n",
    "\n",
    "# Create a PlayerTeamData object\n",
    "merger = MergeData(player_data, team_data)\n",
    "\n",
    "# Call the add_and_flatten method to merge the data and flatten the column index\n",
    "merged_data = merger.add_and_flatten()\n",
    "\n",
    "merged_data['Player'] = merged_data['Player'].str.replace(r'[^\\w\\s]+', '')\n",
    "merged_data = merged_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79b5dc-abde-4fef-8d4c-5a3b949df82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_next_year_ppr(data):\n",
    "    data['next_year_PPR'] = np.nan\n",
    "    \n",
    "    # Group the data by player and sort by year\n",
    "    data_grouped = data.sort_values(by=['Player', 'Year']).groupby('Player')\n",
    "    \n",
    "    # Iterate over each group and calculate next year PPR\n",
    "    for name, group in data_grouped:\n",
    "        group['next_year_PPR'] = group['PPR'].shift(-1)\n",
    "        data.update(group)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089287f-407f-437a-a245-e298851a0293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_ppr_per_game(data):\n",
    "    data['PPR_per_game'] = np.nan\n",
    "    \n",
    "    try:\n",
    "        data['PPR_per_game'] = data['PPR'] / data['G']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b986eef-b038-4414-a961-68507ff1741e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data = add_next_year_ppr(merged_data)\n",
    "merged_data = add_ppr_per_game(merged_data)\n",
    "merged_data = merged_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40e998-d6ba-4f89-97dc-b7615d6edd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0710200-abec-4b98-9db0-57aa234144c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for position in unique positions, create new dataframe with all rows of position.\n",
    "create a loop that iterates over each year, and in each iteration, you train a new LSTM network on the data from that year and all previous years. Then, you can use the trained network to make predictions for the current year. You can save the trained networks for each year, so you can reuse them in future iterations.\n",
    "I want the input data to be all numerical data and the output (predicted value) is next_year_PPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c75a02-82b1-4b27-a4cf-b2f200aad355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
