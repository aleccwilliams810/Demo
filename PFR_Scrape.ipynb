{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0af694-d937-43a4-b347-869467b84654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.feature_selection import RFE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a39918-9d09-4f26-9edb-fc3f479988fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataScraper:\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "        self.player_urls = [f'https://www.pro-football-reference.com/years/{year}/fantasy.htm' for year in years]\n",
    "        self.team_url = 'https://www.pro-football-reference.com/years/{}/'\n",
    "\n",
    "    def scrape_data(self, urls):\n",
    "        data_frames = []\n",
    "        for url in urls:\n",
    "            year = url.split('/')[-2]  # Extract the year from the URL\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            tables = soup.find_all('table')\n",
    "\n",
    "            df_list = [pd.read_html(str(tables[i]))[0] for i in range(min(len(tables), 2))]\n",
    "            if df_list:\n",
    "                df = pd.concat(df_list)\n",
    "                df['Year'] = float(year)\n",
    "                data_frames.append(df)\n",
    "\n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    def scrape_player_data(self):\n",
    "        return self.scrape_data(self.player_urls)\n",
    "\n",
    "    def scrape_team_data(self):\n",
    "        team_urls = [self.team_url.format(year) for year in self.years]\n",
    "        team_data = self.scrape_data(team_urls)\n",
    "        return team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551911f-a955-48d5-abb8-bbd607af9ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \n",
    "    dictionary = {'New York Giants': 'NYG',\n",
    " 'Las Vegas Raiders': 'LVR',\n",
    " 'Los Angeles Chargers': 'LAC',\n",
    " 'Denver Broncos': 'DEN',\n",
    " 'Green Bay Packers': 'GNB',\n",
    " 'Jacksonville Jaguars': 'JAX',\n",
    " 'Washington Redskins': 'WAS',\n",
    " 'Los Angeles Rams': 'LAR',\n",
    " 'Arizona Cardinals': 'ARI',\n",
    " 'Carolina Panthers': 'CAR',\n",
    " 'Baltimore Ravens': 'BAL',\n",
    " 'New York Jets': 'NYJ',\n",
    " 'Miami Dolphins': 'MIA',\n",
    " 'Minnesota Vikings': 'MIN',\n",
    " 'Oakland Raiders': 'OAK',\n",
    " 'Chicago Bears': 'CHI',\n",
    " 'New England Patriots': 'NWE',\n",
    " 'Tennessee Titans': 'TEN',\n",
    " 'New Orleans Saints': 'NOR',\n",
    " 'Cleveland Browns': 'CLE',\n",
    " 'Tampa Bay Buccaneers': 'TAM',\n",
    " 'Buffalo Bills': 'BUF',\n",
    " 'Cincinnati Bengals': 'CIN',\n",
    " 'Houston Texans': 'HOU',\n",
    " 'San Francisco 49ers': 'SFO',\n",
    " 'Atlanta Falcons': 'ATL',\n",
    " 'Washington Football Team': 'WAS',\n",
    " 'Indianapolis Colts': 'IND',\n",
    " 'Seattle Seahawks': 'SEA',\n",
    " 'Pittsburgh Steelers': 'PIT',\n",
    " 'Dallas Cowboys': 'DAL',\n",
    " 'Detroit Lions': 'DET',\n",
    " 'Philadelphia Eagles': 'PHI',\n",
    " 'Kansas City Chiefs': 'KAN'}\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def try_convert_to_float(self, x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            if x == '' or pd.isna(x):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    def convert_columns_to_float(self):\n",
    "        # Convert all columns to float if possible\n",
    "        self.data = self.data.applymap(self.try_convert_to_float)\n",
    "        self.data = self.data.astype(float, errors='ignore')\n",
    "\n",
    "    def flatten_multiindex_header(self):\n",
    "        # Flatten multi-level column index to single level column index if it exists\n",
    "        if isinstance(self.data.columns, pd.MultiIndex):\n",
    "            level1 = self.data.columns.get_level_values(0)\n",
    "            level2 = self.data.columns.get_level_values(1)\n",
    "\n",
    "            # Count duplicates in level2\n",
    "            duplicates = level2.value_counts() > 1\n",
    "\n",
    "            # Create new column names\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if duplicates[col_level2]:\n",
    "                    new_columns.append(f'{col_level1}_{col_level2}')\n",
    "                elif col_level2 == '':\n",
    "                    new_columns.append(col_level1)\n",
    "                else:\n",
    "                    new_columns.append(col_level2)\n",
    "\n",
    "            # Replace the MultiIndex header with the new flattened header\n",
    "            self.data.columns = new_columns\n",
    "        return self.data\n",
    " \n",
    "    def calculate_yards_per_attempt(self):\n",
    "    # Calculate yards per rushing attempt\n",
    "        if 'Rushing_Att' in self.data.columns and 'Rushing_Yds' in self.data.columns:\n",
    "            self.data['Y/A'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rushing_Att']) or x['Rushing_Att'] == 0) else x['Rushing_Yds'] / x['Rushing_Att'], axis=1)\n",
    "\n",
    "    def calculate_yards_per_reception(self):\n",
    "    # Calculate yards per reception\n",
    "        if 'Rec' in self.data.columns and 'Receiving_Yds' in self.data.columns:\n",
    "            self.data['Y/R'] = self.data.apply(lambda x: 0 if (pd.isna(x['Rec']) or x['Rec'] == 0) else x['Receiving_Yds'] / x['Rec'], axis=1)\n",
    "            \n",
    "    def handle_missing_values(self, thresh=0.5):\n",
    "    \n",
    "        # Drop rows where most of the columns have string data\n",
    "        self.data = self.data[~(np.sum(np.vectorize(isinstance)(self.data.values, str), axis=1) > thresh * len(self.data.columns))]\n",
    "\n",
    "        def should_fill(col):\n",
    "            na_and_zero_count = (col.apply(lambda x: pd.isna(x) or x == 0)).sum()\n",
    "            return na_and_zero_count / len(col) >= 0.7\n",
    "\n",
    "        should_fill_mask = self.data.apply(should_fill)\n",
    "        fill_mask = self.data.apply(lambda col: col.isna() & should_fill_mask[col.name])\n",
    "        self.data = self.data.mask(fill_mask, 0.0)\n",
    "\n",
    "        # Drop columns where most of the rows are null\n",
    "        self.data = self.data.drop(columns=self.data.columns[self.data.isnull().mean() > thresh], errors='ignore')\n",
    "\n",
    "        # Drop rows where most of the columns are null\n",
    "        self.data = self.data.dropna(thresh=thresh * len(self.data.columns))\n",
    "\n",
    "        # Drop rows where PPR data is null (if present in dataframe)\n",
    "        if 'PPR' in self.data.columns:\n",
    "            self.data = self.data.dropna(subset=['PPR'])\n",
    "\n",
    "         # Drop rows where Rk > 350 if Rk is a column\n",
    "        if 'Rk' in self.data.columns:\n",
    "            self.data = self.data[self.data['Rk'] <= 400]\n",
    "\n",
    "    def replace_team_names(self, dictionary):\n",
    "        for key, value in dictionary.items():\n",
    "            mask = self.data['Tm'].str.startswith(key)\n",
    "            if mask.any():\n",
    "                self.data.loc[mask, 'Tm'] = value\n",
    "        return self.data\n",
    "        \n",
    "    def feature_engineering(self):\n",
    "        # Create new features or modify existing ones based on domain knowledge\n",
    "        pass\n",
    "\n",
    "    def feature_scaling(self):\n",
    "        # Scale numerical features to a standard range, e.g., using Min-Max scaling or StandardScaler from sklearn\n",
    "        pass\n",
    "\n",
    "    def normalize_data(self):\n",
    "        # Normalize data to reduce the impact of outliers or skewed distributions, e.g., using log transformation or Box-Cox transformation\n",
    "        pass\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.convert_columns_to_float()\n",
    "        self.flatten_multiindex_header()\n",
    "        self.handle_missing_values()\n",
    "        self.replace_team_names(dictionary=self.dictionary)  # pass dictionary argument\n",
    "        self.calculate_yards_per_attempt()\n",
    "        self.calculate_yards_per_reception()\n",
    "        # Call other preprocessing methods in the correct order\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80ff8c-54cd-40e7-a6c1-4da66f8a26d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MergeAndProcess:\n",
    "    def __init__(self, player_data, team_data):\n",
    "        self.player_data = player_data\n",
    "        self.team_data = team_data\n",
    "\n",
    "    def merge(self):\n",
    "        # Create a dictionary to map team data to new columns in player data\n",
    "        team_data_map = {}\n",
    "        for col in self.team_data.columns:\n",
    "            if col not in ['Year', 'Tm']:\n",
    "                if col in self.player_data.columns:\n",
    "                    team_data_map[col] = col\n",
    "                else:\n",
    "                    team_data_map[col] = f'Team_{col}'\n",
    "\n",
    "        # Create new columns in player data for team data\n",
    "        for col in team_data_map.values():\n",
    "            self.player_data[col] = np.nan\n",
    "\n",
    "        # Map team data to new columns in player data based on team and year\n",
    "        self.player_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.team_data.set_index(['Year', 'Tm'], inplace=True)\n",
    "        self.player_data.update(self.team_data.rename(columns=team_data_map))\n",
    "\n",
    "        # Flatten the column index\n",
    "        if isinstance(self.player_data.columns, pd.MultiIndex):\n",
    "            level1 = self.player_data.columns.get_level_values(0)\n",
    "            level2 = self.player_data.columns.get_level_values(1)\n",
    "            new_columns = []\n",
    "            for col_level1, col_level2 in zip(level1, level2):\n",
    "                if col_level1.startswith('Team_'):\n",
    "                    new_columns.append(col_level2)\n",
    "                else:\n",
    "                    new_columns.append(col_level1)\n",
    "            self.player_data.columns = new_columns\n",
    "\n",
    "        return self.player_data.reset_index()\n",
    "\n",
    "    def process(self):\n",
    "        \n",
    "        # Merge player and team data\n",
    "        merged_data = self.merge()\n",
    "        \n",
    "        # Add next year PPR\n",
    "        merged_data['next_year_PPR'] = merged_data.groupby('Player')['PPR'].shift(-1)\n",
    "\n",
    "        # Add PPR per game\n",
    "        merged_data['PPR_per_game'] = np.nan\n",
    "        try:\n",
    "            merged_data['PPR_per_game'] = merged_data['PPR'] / merged_data['G']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Convert column types\n",
    "        for col in merged_data.columns:\n",
    "            if merged_data[col].dtype == 'object':\n",
    "                try:\n",
    "                    merged_data[col] = pd.to_numeric(merged_data[col], errors='raise').astype('float')\n",
    "                except ValueError:\n",
    "                    merged_data[col] = merged_data[col].astype('string')\n",
    "            else:\n",
    "                dtype = merged_data[col].dtype\n",
    "                merged_data[col] = merged_data[col].astype(dtype)\n",
    "\n",
    "        # Drop missing values\n",
    "        merged_data = merged_data.dropna()\n",
    "        \n",
    "        # Reset index\n",
    "        merged_data = merged_data.reset_index(drop=True)\n",
    "        \n",
    "        # Replace non-alphanumeric characters in player names\n",
    "        merged_data['Player'] = merged_data['Player'].str.replace(r'[^\\w\\s]+', '')\n",
    "        \n",
    "        return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77815273-1c01-40d0-804a-6515cf0825fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "years = list(range(2013, 2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6f45c-4d9c-4d37-a74c-cad5733bc734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scraper = DataScraper(years)\n",
    "player_data = scraper.scrape_player_data()\n",
    "team_data = scraper.scrape_team_data()\n",
    "\n",
    "player_preprocessor = DataPreprocessor(player_data)\n",
    "player_data = player_preprocessor.preprocess_data()\n",
    "\n",
    "team_preprocessor = DataPreprocessor(team_data)\n",
    "team_data = team_preprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2f767-1094-477c-9268-662d8f97d85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create instance of MergeAndProcess class\n",
    "merger = MergeAndProcess(player_data, team_data)\n",
    "\n",
    "# call process_data method to merge and process data\n",
    "merged_data = merger.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4469baf-6612-499d-bea5-49d68381397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the merged_data DataFrame into training/validation (2015-2020) and testing (2021)\n",
    "train_val_data = merged_data[merged_data['Year'] < merged_data['Year'].max()]\n",
    "test_data = merged_data[merged_data['Year'] == merged_data['Year'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec244b2-8043-4781-a320-93b21a9bdc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e94435-0233-4cf0-b8be-68f2d1dc2e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d331f4-5190-4771-bb31-4a42a35d2bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_autoencoder_data(data):\n",
    "    # Remove non-numeric columns\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Normalize numeric data using Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    return normalized_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380bbdb-77a0-43d1-a5ab-3a5723d7a598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def autoencoder(input_dim, encoding_dim=64):\n",
    "    # Create the input layer with the specified input_dim\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # encoder\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "    # decoder \n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "    # Create the encoder model (this will be used later for encoding the data)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    # Compile the autoencoder model\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e926c7-52cf-437d-a908-47c24ba4cc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the unique positions in the dataset\n",
    "positions = merged_data['FantPos'].unique()\n",
    "\n",
    "# Create a dictionary to store data for each position\n",
    "position_dfs = {position: train_val_data[train_val_data['FantPos'] == position] for position in positions}\n",
    "\n",
    "# Initialize dictionaries to store models and performance metrics\n",
    "autoencoders = {}\n",
    "regression_models = {}\n",
    "mae_scores = {}\n",
    "\n",
    "for position in positions:\n",
    "    # Get the position-specific data\n",
    "    train_val_data_pos = position_dfs[position]\n",
    "    test_data_pos = test_data[test_data['FantPos'] == position]\n",
    "\n",
    "    ## Use RFE to select the top 10 features for the position-specific data\n",
    "    X = train_val_data_pos.drop(['next_year_PPR'], axis=1)\n",
    "    X = X.select_dtypes(exclude=['string']) # exclude columns with string data\n",
    "    y = train_val_data_pos['next_year_PPR']\n",
    "\n",
    "    rfe = RFE(RandomForestRegressor(), n_features_to_select=20)\n",
    "    rfe.fit(X, y)\n",
    "    top_20_features = X.columns[rfe.support_].tolist()\n",
    "    X = X[top_20_features]\n",
    "    # Modify the train_val_data_pos and test_data_pos to include only the top features\n",
    "    train_val_data_pos = train_val_data_pos[['PPR', 'next_year_PPR'] + top_20_features]\n",
    "    test_data_pos = test_data_pos[['PPR', 'next_year_PPR'] + top_20_features]\n",
    "\n",
    "    # Preprocess the data for the autoencoder\n",
    "    train_val_normalized_pos, train_val_scaler_pos = preprocess_autoencoder_data(train_val_data_pos)\n",
    "    test_normalized_pos, test_scaler_pos = preprocess_autoencoder_data(test_data_pos)\n",
    "    \n",
    "    # Define the autoencoder architecture for the position\n",
    "    autoencoder_pos, encoder_pos = autoencoder(train_val_normalized_pos.shape[1]) \n",
    "    \n",
    "    # Train the autoencoder using the position-specific train_val_normalized dataset\n",
    "    autoencoder_pos.fit(train_val_normalized_pos, train_val_normalized_pos, epochs=100, batch_size=32)\n",
    "\n",
    "    # Encode the position-specific train_val_normalized and test_normalized datasets\n",
    "    train_val_encoded_pos = encoder_pos.predict(train_val_normalized_pos)\n",
    "    test_encoded_pos = encoder_pos.predict(test_normalized_pos)\n",
    "\n",
    "    # Train a regression model using the encoded representation of the position-specific train_val_normalized dataset\n",
    "    regression_model_pos = RandomForestRegressor()\n",
    "    regression_model_pos.fit(train_val_encoded_pos, train_val_data_pos['next_year_PPR'])\n",
    "\n",
    "    # Evaluate the model's performance on the encoded position-specific test_normalized dataset\n",
    "    predictions_pos = regression_model_pos.predict(test_encoded_pos)\n",
    "    actual_values_pos = test_data_pos['next_year_PPR']\n",
    "\n",
    "    # Calculate evaluation metrics, e.g., mean_absolute_error\n",
    "    mae_pos = mean_absolute_error(actual_values_pos, predictions_pos)\n",
    "\n",
    "    # Save the autoencoder, regression model, and performance metrics for the position\n",
    "    autoencoders[position] = autoencoder_pos\n",
    "    regression_models[position] = regression_model_pos\n",
    "    mae_scores[position] = mae_pos\n",
    "\n",
    "# Print the evaluation metrics for each position\n",
    "for position, mae_pos in mae_scores.items():\n",
    "    print(f'Mean Absolute Error for {position}: {mae_pos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45f0e2-d277-4707-ae9a-078a425c05e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the test data and predictions\n",
    "all_test_data = pd.DataFrame()\n",
    "\n",
    "# Iterate over each position in the positions list\n",
    "for position in positions:\n",
    "    # Get the position-specific test_normalized dataset\n",
    "    test_normalized_pos = test_normalized[position]\n",
    "\n",
    "    # Encode the position-specific test_normalized dataset\n",
    "    encoder_pos = autoencoders[position].get_layer(index=1) # Get the encoder from the autoencoder\n",
    "    test_encoded_pos = encoder_pos(test_normalized_pos)\n",
    "    \n",
    "    # Use the trained regression model to predict the next_year_PPR values\n",
    "    predictions_pos = regression_models[position].predict(test_encoded_pos)\n",
    "\n",
    "    # Add the predictions to the position-specific test_data dataframe\n",
    "    test_data_pos = test_data[position]\n",
    "    test_data_pos['predictions'] = predictions_pos\n",
    "\n",
    "    # Append the position-specific test_data dataframe to the combined test_data dataframe\n",
    "    all_test_data = all_test_data.append(test_data_pos)\n",
    "\n",
    "# Merge the all_test_data dataframe with the original merged_data dataframe\n",
    "merged_data_with_predictions = pd.merge(merged_data, all_test_data[['predictions']], left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Visualize the predictions against the actual values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='next_year_PPR', y='predictions', data=merged_data_with_predictions, hue='FantPos', palette='viridis')\n",
    "plt.xlabel('Actual Next Year PPR')\n",
    "plt.ylabel('Predicted Next Year PPR')\n",
    "plt.title('Predicted vs. Actual Next Year PPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46050817-a346-4183-bc79-97c4358207fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
